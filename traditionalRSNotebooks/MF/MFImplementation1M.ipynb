{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "german-legislation",
   "metadata": {},
   "source": [
    "# Import the packages and check connection to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "biological-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd ## for dataset and eda\n",
    "import numpy as np ## for eda\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "clear-scholar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Bucket: firstprojectdl>\n",
      "Great, we now have access to our first bucket on google cloud storage where we put our data\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"firstprojectdl\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "print(bucket)\n",
    "print('Great, we now have access to our first bucket on google cloud storage where we put our data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-consumer",
   "metadata": {},
   "source": [
    "RATINGS FILE DESCRIPTION\n",
    "================================================================================\n",
    "\n",
    "All ratings are contained in the file \"ratings.dat\" and are in the\n",
    "following format:\n",
    "\n",
    "UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "- UserIDs range between 1 and 6040 \n",
    "- MovieIDs range between 1 and 3952\n",
    "- Ratings are made on a 5-star scale (whole-star ratings only)\n",
    "- Timestamp is represented in seconds since the epoch as returned by time(2)\n",
    "- Each user has at least 20 ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-recipient",
   "metadata": {},
   "source": [
    "MOVIES FILE DESCRIPTION\n",
    "================================================================================\n",
    "\n",
    "Movie information is in the file \"movies.dat\" and is in the following\n",
    "format:\n",
    "\n",
    "MovieID::Title::Genres\n",
    "\n",
    "- Titles are identical to titles provided by the IMDB (including\n",
    "year of release)\n",
    "- Genres are pipe-separated and are selected from the following genres:\n",
    "\n",
    "\t* Action\n",
    "\t* Adventure\n",
    "\t* Animation\n",
    "\t* Children's\n",
    "\t* Comedy\n",
    "\t* Crime\n",
    "\t* Documentary\n",
    "\t* Drama\n",
    "\t* Fantasy\n",
    "\t* Film-Noir\n",
    "\t* Horror\n",
    "\t* Musical\n",
    "\t* Mystery\n",
    "\t* Romance\n",
    "\t* Sci-Fi\n",
    "\t* Thriller\n",
    "\t* War\n",
    "\t* Western\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-interface",
   "metadata": {},
   "source": [
    "# Read the datasets from google cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "gross-enzyme",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "\n",
    "bucket_name = \"firstprojectdl\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "# When you have your files in a subfolder of the bucket.\n",
    "my_prefix = \"data/movieLens/movieLens1M/\" # the name of the subfolder\n",
    "blobs = bucket.list_blobs(prefix = my_prefix, delimiter = '/')\n",
    "\n",
    "dfDict = {}\n",
    "dateparse = lambda x: datetime.utcfromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "for blob in blobs:\n",
    "    if(blob.name != my_prefix): # ignoring the subfolder itself \n",
    "        file_name = blob.name.replace(my_prefix, \"\")\n",
    "        blob.download_to_filename(file_name) # download the file to the machine\n",
    "        if file_name =='ratings.dat':\n",
    "            df = pd.read_csv('ratings.dat', sep = '::', header=None)\n",
    "            df.columns = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "            dfDict[file_name] = df\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "dfDict['rating'] = dfDict['ratings.dat'] \n",
    "del dfDict['ratings.dat'] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "forward-zealand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000209, 4)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfDict['rating']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "mineral-seminar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-12-31 22:12:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-31 22:35:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-31 22:32:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-12-31 22:04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>2001-01-06 23:38:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating           timestamp\n",
       "0        1      1193       5 2000-12-31 22:12:40\n",
       "1        1       661       3 2000-12-31 22:35:09\n",
       "2        1       914       3 2000-12-31 22:32:48\n",
       "3        1      3408       4 2000-12-31 22:04:35\n",
       "4        1      2355       5 2001-01-06 23:38:11"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "national-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 6040\n",
      "Number of items/movies rated: 3706\n"
     ]
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "print(f'Number of users: {n_users}')\n",
    "n_items = df.movie_id.unique().shape[0]\n",
    "print(f'Number of items/movies rated: {n_items}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "progressive-norfolk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Matrix shape is (6040, 3706)\n",
      "Maximum number of ratings is 22384240. With this number of ratings, the matrix would be dense\n"
     ]
    }
   ],
   "source": [
    "## then create the rating matrix with dimension (number of users, number of items)\n",
    "\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "print(ratings)\n",
    "print(f'Matrix shape is {ratings.shape}')\n",
    "maxRatingsPossible = n_users * n_items\n",
    "print(f'Maximum number of ratings is {maxRatingsPossible}. With this number of ratings, the matrix would be dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "expanded-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "listMovies = df.movie_id.unique()\n",
    "for newId, movies in enumerate(listMovies):\n",
    "    df.loc[df['movie_id'] == movies, 'movie_id'] = newId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "hairy-filter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000209it [00:02, 490656.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 5.],\n",
       "       [0., 0., 0., ..., 0., 0., 5.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 4.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## what we do here here is as follows:\n",
    "# 1. we iterate over the dataframe rows\n",
    "# 2. we then go to the right user id row\n",
    "# and rating column and then assign it the right rating\n",
    "from tqdm import tqdm\n",
    "for row in tqdm(df.itertuples()):\n",
    "    ratings[row[1]-1, row[2]-1] = row[3] \n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "outstanding-shepherd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "devoted-limit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Sparsity : 4.219 %\n"
     ]
    }
   ],
   "source": [
    "numerator = float(len(ratings.nonzero()[0])) # of course, we have 100,000 ratings\n",
    "denominator =(ratings.shape[0] * ratings.shape[1]) # then 100000/1586126 \n",
    "\n",
    "sparsity = numerator/denominator\n",
    "\n",
    "sparsity *= 100 # get the percentage\n",
    "\n",
    "print(f'Matrix Sparsity : {round(sparsity, 3)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "remarkable-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(ratings):\n",
    "    \"\"\"\n",
    "    The purpose of this method is split our datasets betwen:\n",
    "    1. Train\n",
    "    2. Validation (we extracted 7 ratings for a user from train and put it in there)\n",
    "    3. Test (we extracted 7 ratings for a user from train and put it in there)\n",
    "    We go through each of the user and randonly select ratings from train set that will go\n",
    "    from the initial matrix, take 7 to put in the validation matrix \n",
    "    and take 7 to put in the test matrix. Finally these ratings that have been put in \n",
    "    the test and val sets are set to 0 in the main/train matrix\n",
    "    \"\"\"\n",
    "    test = np.zeros(ratings.shape)\n",
    "    validation = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                       size=7, \n",
    "                                        replace=False) # for user, get the test rating array\n",
    "        val_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                       size=7, \n",
    "                                        replace=False) # for user, get the validation array\n",
    "        train[user, test_ratings] = 0. # set to 0 the train matrix/copy of the original matrix the ratings taken for test\n",
    "        train[user, val_ratings] = 0.# set to 0 the train matrix/copy of the original matrix the ratings taken for validation\n",
    "        test[user, test_ratings] = ratings[user, test_ratings] # assign to the test matrix/matrix of 0 the test ratings\n",
    "        validation[user, val_ratings] = ratings[user, val_ratings] # assign to the val matrix/matrix of 0 the val ratings\n",
    "        \n",
    "    assert(np.all((train * test * validation) == 0))\n",
    "\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "alone-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the train rating matrix\n",
      "[[5. 0. 0. ... 0. 0. 5.]\n",
      " [0. 0. 0. ... 0. 0. 5.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 4.]]\n",
      "The shape is (6040, 3706)\n",
      "This is the validation rating matrix\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "The shape is (6040, 3706)\n",
      "This is the test rating matrix\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "The shape is (6040, 3706)\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = train_val_test_split(ratings)\n",
    "\n",
    "print(\"This is the train rating matrix\")\n",
    "print(train) \n",
    "print(f\"The shape is {train.shape}\")\n",
    "print(\"This is the validation rating matrix\")\n",
    "print(validation)\n",
    "print(f\"The shape is {validation.shape}\")\n",
    "print(\"This is the test rating matrix\")\n",
    "print(test)\n",
    "print(f\"The shape is {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "generic-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "after-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFals():\n",
    "    def __init__(self, \n",
    "                 ratings, \n",
    "                 n_factors=40, \n",
    "                 item_reg=0.0, \n",
    "                 user_reg=0.0,\n",
    "                 verbose=False\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        \n",
    "        item_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors ## our k number of attributes!!\n",
    "        self.item_reg = item_reg\n",
    "        self.user_reg = user_reg\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs) # the Y times Y transpose part of the derived function equal to 0\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda # the second argument to get the user latent vectors\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs)) ## here what we do is search for the latent vectors for each of the user u\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs) # same calculation as above but with user vector\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs)) ## here what we do is search for the latent vectors for each of the movie i in the column!\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10):\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors\n",
    "        self.user_vecs = np.random.random((self.n_users, self.n_factors)) # random start for our user vecs and item vecs before we improve them with the ALS\n",
    "        self.item_vecs = np.random.random((self.n_items, self.n_factors))\n",
    "        \n",
    "        self.partial_train(n_iter)\n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        Get the user_vecs and item_vecs latent vectors\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 10 == 0 and self._v:\n",
    "                print(f'current iteration: {ctr}')\n",
    "            self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                           self.item_vecs, \n",
    "                                           self.ratings, \n",
    "                                           self.user_reg, \n",
    "                                           type='user') # our user latent vector\n",
    "            self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                           self.user_vecs, \n",
    "                                           self.ratings, \n",
    "                                           self.item_reg, \n",
    "                                           type='item') # our item latent vactor\n",
    "            ctr += 1\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item. \"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction. \"\"\"\n",
    "        return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, val, training=True):\n",
    "        \"\"\"\n",
    "        Keep track of RMSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        val : (2D ndarray)\n",
    "            Validation dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_rmse : (list)\n",
    "            Training data RMSE values for each value of iter_array\n",
    "        val_rmse : (list)\n",
    "            val data RMSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        \n",
    "        if training:\n",
    "            iter_array.sort()\n",
    "            self.train_rmse =[]\n",
    "            self.val_rmse = []\n",
    "            iter_diff = 0\n",
    "            for (i, n_iter) in enumerate(iter_array):\n",
    "                if self._v:\n",
    "                    print('Iteration: {n_iter}')\n",
    "                if i == 0:\n",
    "                    self.train(n_iter - iter_diff)\n",
    "                else:\n",
    "                    self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "                predictions = self.predict_all()\n",
    "\n",
    "                self.train_rmse += [get_rmse(predictions, self.ratings)]\n",
    "                self.val_rmse += [get_rmse(predictions, val)]\n",
    "                if self._v:\n",
    "                    print('Train rmse: ' + str(self.train_rmse[-1]))\n",
    "                    print('Validation rmse: ' + str(self.val_rmse[-1]))\n",
    "                iter_diff = n_iter\n",
    "                \n",
    "        if training == False:\n",
    "            iter_array.sort()\n",
    "            self.test_rmse = []\n",
    "            iter_diff = 0\n",
    "            for (i, n_iter) in enumerate(iter_array):\n",
    "            \n",
    "                predictions = self.predict_all()\n",
    "\n",
    "                self.test_rmse += [get_rmse(predictions, val)]\n",
    "\n",
    "                iter_diff = n_iter\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "attempted-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_rmse(pred, actual):\n",
    "    \"\"\" The goal of this function is to get the RMSE \n",
    "    to assess the performance of the learner\n",
    "    \"\"\"\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "incorporated-somerset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 1s, sys: 548 ms, total: 6min 1s\n",
      "Wall time: 4min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MF_als = MFals(train, n_factors=40, user_reg=0.0, item_reg=0.0)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_als.calculate_learning_curve(iter_array, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "departmental-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "def plot_learning_curve(iter_array, model):\n",
    "    plt.plot(iter_array, model.train_rmse, \\\n",
    "             label='Training', linewidth=5)\n",
    "    plt.plot(iter_array, model.val_rmse, \\\n",
    "             label='Validation', linewidth=5)\n",
    "\n",
    "\n",
    "    plt.xticks(fontsize=16);\n",
    "    plt.yticks(fontsize=16);\n",
    "    plt.xlabel('iterations', fontsize=30);\n",
    "    plt.ylabel('RMSE', fontsize=30);\n",
    "    plt.legend(loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(iter_array, MF_als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MF_als.calculate_learning_curve(iter_array, test, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curveTest(iter_array, model):\n",
    "    plt.plot(iter_array, model.val_rmse, \\\n",
    "             label='Validation', linewidth=5)\n",
    "    plt.plot(iter_array, model.test_rmse, \\\n",
    "             label='Test', linewidth=5)\n",
    "\n",
    "\n",
    "    plt.xticks(fontsize=16);\n",
    "    plt.yticks(fontsize=16);\n",
    "    plt.xlabel('iterations', fontsize=30);\n",
    "    plt.ylabel('RMSE', fontsize=30);\n",
    "    plt.legend(loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curveTest(iter_array, MF_als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MF_alsOpt = MFals(train, n_factors=40, \\\n",
    "                    user_reg=30., item_reg=30.)\n",
    "\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_alsOpt.calculate_learning_curve(iter_array, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(iter_array, MF_alsOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-spice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
