{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.load_data import Data\n",
    "from utils.helper_functions import early_stopping,\\\n",
    "                                   train,\\\n",
    "                                   split_matrix,\\\n",
    "                                   compute_ndcg_k,\\\n",
    "                                   eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim, layers, reg, node_dropout, mess_dropout,\n",
    "        adj_mtx):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize Class attributes\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.adj_mtx = adj_mtx\n",
    "        self.laplacian = adj_mtx - sp.eye(adj_mtx.shape[0])\n",
    "        self.reg = reg\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(self.layers)\n",
    "        self.node_dropout = node_dropout\n",
    "        self.mess_dropout = mess_dropout\n",
    "\n",
    "        #self.u_g_embeddings = nn.Parameter(torch.empty(n_users, emb_dim+np.sum(self.layers)))\n",
    "        #self.i_g_embeddings = nn.Parameter(torch.empty(n_items, emb_dim+np.sum(self.layers)))\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weight_dict = self._init_weights()\n",
    "        print(\"Weights initialized.\")\n",
    "\n",
    "        # Create Matrix 'A', PyTorch sparse tensor of SP adjacency_mtx\n",
    "        self.A = self._convert_sp_mat_to_sp_tensor(self.adj_mtx)\n",
    "        self.L = self._convert_sp_mat_to_sp_tensor(self.laplacian)\n",
    "\n",
    "    # initialize weights\n",
    "    def _init_weights(self):\n",
    "        print(\"Initializing weights...\")\n",
    "        weight_dict = nn.ParameterDict()\n",
    "\n",
    "        initializer = torch.nn.init.xavier_uniform_\n",
    "        \n",
    "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\n",
    "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\n",
    "\n",
    "        weight_size_list = [self.emb_dim] + self.layers\n",
    "      \n",
    "        for k in range(self.n_layers):\n",
    "            weight_dict['W_gc_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
    "            weight_dict['b_gc_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
    "            \n",
    "            weight_dict['W_bi_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
    "            weight_dict['b_bi_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
    "           \n",
    "        return weight_dict\n",
    "\n",
    "    # convert sparse matrix into sparse PyTorch tensor\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"\n",
    "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        X = Adjacency matrix, scipy sparse matrix\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
    "        v = torch.FloatTensor(coo.data)\n",
    "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
    "        return res\n",
    "\n",
    "    # apply node_dropout\n",
    "    def _droupout_sparse(self, X):\n",
    "        \"\"\"\n",
    "        Drop individual locations in X\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        X = adjacency matrix (PyTorch sparse tensor)\n",
    "        dropout = fraction of nodes to drop\n",
    "        noise_shape = number of non non-zero entries of X\n",
    "        \"\"\"\n",
    "        \n",
    "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\n",
    "        i = X.coalesce().indices()\n",
    "        v = X.coalesce()._values()\n",
    "        i[:,node_dropout_mask] = 0\n",
    "        v[node_dropout_mask] = 0\n",
    "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
    "\n",
    "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        \"\"\"\n",
    "        Computes the forward pass\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        u = user\n",
    "        i = positive item (user interacted with item)\n",
    "        j = negative item (user did not interact with item)\n",
    "        \"\"\"\n",
    "        # apply drop-out mask\n",
    "        A_hat = self._droupout_sparse(self.A) if self.node_dropout > 0 else self.A\n",
    "        L_hat = self._droupout_sparse(self.L) if self.node_dropout > 0 else self.L\n",
    "\n",
    "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
    "\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        # forward pass for 'n' propagation layers\n",
    "        for k in range(self.n_layers):\n",
    "\n",
    "            # weighted sum messages of neighbours\n",
    "            side_embeddings = torch.sparse.mm(A_hat, ego_embeddings)\n",
    "            side_L_embeddings = torch.sparse.mm(L_hat, ego_embeddings)\n",
    "\n",
    "            # transformed sum weighted sum messages of neighbours\n",
    "            sum_embeddings = torch.matmul(side_embeddings, self.weight_dict['W_gc_%d' % k]) + self.weight_dict['b_gc_%d' % k]\n",
    "\n",
    "            # bi messages of neighbours\n",
    "            bi_embeddings = torch.mul(ego_embeddings, side_L_embeddings)\n",
    "            # transformed bi messages of neighbours\n",
    "            bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_bi_%d' % k]) + self.weight_dict['b_bi_%d' % k]\n",
    "\n",
    "            # non-linear activation \n",
    "            ego_embeddings = F.leaky_relu(sum_embeddings + bi_embeddings)\n",
    "            # + message dropout\n",
    "            mess_dropout_mask = nn.Dropout(self.mess_dropout)\n",
    "            ego_embeddings = mess_dropout_mask(ego_embeddings)\n",
    "\n",
    "            # normalize activation\n",
    "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "\n",
    "            all_embeddings.append(norm_embeddings)\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, 1)\n",
    "        \n",
    "        # back to user/item dimension\n",
    "        u_g_embeddings, i_g_embeddings = all_embeddings.split([self.n_users, self.n_items], 0)\n",
    "\n",
    "        self.u_g_embeddings = nn.Parameter(u_g_embeddings)\n",
    "        self.i_g_embeddings = nn.Parameter(i_g_embeddings)\n",
    "        \n",
    "        u_emb = u_g_embeddings[u] # user embeddings\n",
    "        p_emb = i_g_embeddings[i] # positive item embeddings\n",
    "        n_emb = i_g_embeddings[j] # negative item embeddings\n",
    "\n",
    "        y_ui = torch.mul(u_emb, p_emb).sum(dim=1)\n",
    "        y_uj = torch.mul(u_emb, n_emb).sum(dim=1)\n",
    "        log_prob = (torch.log(torch.sigmoid(y_ui-y_uj))).mean()\n",
    "\n",
    "        # compute bpr-loss\n",
    "        bpr_loss = -log_prob\n",
    "        if self.reg > 0.:\n",
    "            l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\n",
    "            l2reg  = self.reg*l2norm\n",
    "            bpr_loss =  -log_prob + l2reg\n",
    "\n",
    "        return bpr_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=943, n_items=1682\n",
      "n_interactions=100000\n",
      "n_train=80064, n_test=19936, sparsity=0.06305\n",
      "Creating interaction matrices R_train and R_test...\n",
      "Complete. Interaction matrices R_train and R_test created in 0.8148708343505859 sec\n",
      "Loaded adjacency-matrix (shape: (2625, 2625) ) in 0.00932002067565918 sec.\n"
     ]
    }
   ],
   "source": [
    "batch_size=1024\n",
    "data_dir='./data/'\n",
    "dataset='ml-100k'\n",
    "emb_dim=64\n",
    "eval_N=1\n",
    "k=20\n",
    "layers=[64]\n",
    "lr=0.0001\n",
    "mess_dropout=0.1\n",
    "n_epochs=400\n",
    "node_dropout=0.0\n",
    "reg=1e-05\n",
    "esults_dir='results'\n",
    "save_results=1\n",
    "\n",
    "# generate the NGCF-adjacency matrix\n",
    "data_generator = Data(path=data_dir + dataset, batch_size=batch_size)\n",
    "adj_mtx = data_generator.get_adj_mat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yichen/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py:550: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights initialized.\n"
     ]
    }
   ],
   "source": [
    "# create model name and save\n",
    "modelname =  \"NGCF\" + \\\n",
    "        \"_bs_\" + str(batch_size) + \\\n",
    "        \"_nemb_\" + str(emb_dim) + \\\n",
    "        \"_layers_\" + str(layers) + \\\n",
    "        \"_nodedr_\" + str(node_dropout) + \\\n",
    "        \"_messdr_\" + str(mess_dropout) + \\\n",
    "        \"_reg_\" + str(reg) + \\\n",
    "        \"_lr_\"  + str(lr)\n",
    "\n",
    "# create NGCF model\n",
    "model = NGCF(data_generator.n_users, \n",
    "                 data_generator.n_items,\n",
    "                 emb_dim,\n",
    "                 layers,\n",
    "                 reg,\n",
    "                 node_dropout,\n",
    "                 mess_dropout,\n",
    "                 adj_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2021-04-10 21:40:29.014338\n",
      "Using cuda for computations\n",
      "Params on CUDA: True\n",
      "Epoch: 0, Training time: 2.64s, Loss: 54.7800\n",
      "Evaluate current model:\n",
      " Epoch: 0, Validation time: 0.69s \n",
      " Loss: 54.7800: \n",
      " Recall@20: 0.0103 \n",
      " NDCG@20: 0.0743\n",
      "Epoch: 1, Training time: 2.64s, Loss: 54.6990\n",
      "Evaluate current model:\n",
      " Epoch: 1, Validation time: 0.65s \n",
      " Loss: 54.6990: \n",
      " Recall@20: 0.0112 \n",
      " NDCG@20: 0.0778\n",
      "Epoch: 2, Training time: 2.59s, Loss: 54.6064\n",
      "Evaluate current model:\n",
      " Epoch: 2, Validation time: 0.66s \n",
      " Loss: 54.6064: \n",
      " Recall@20: 0.0109 \n",
      " NDCG@20: 0.0759\n",
      "Epoch: 3, Training time: 2.69s, Loss: 54.5517\n",
      "Evaluate current model:\n",
      " Epoch: 3, Validation time: 0.68s \n",
      " Loss: 54.5517: \n",
      " Recall@20: 0.0140 \n",
      " NDCG@20: 0.0892\n",
      "Epoch: 4, Training time: 2.61s, Loss: 54.4445\n",
      "Evaluate current model:\n",
      " Epoch: 4, Validation time: 0.65s \n",
      " Loss: 54.4445: \n",
      " Recall@20: 0.0141 \n",
      " NDCG@20: 0.0881\n",
      "Epoch: 5, Training time: 2.61s, Loss: 54.3108\n",
      "Evaluate current model:\n",
      " Epoch: 5, Validation time: 0.62s \n",
      " Loss: 54.3108: \n",
      " Recall@20: 0.0161 \n",
      " NDCG@20: 0.0969\n",
      "Epoch: 6, Training time: 2.63s, Loss: 54.1072\n",
      "Evaluate current model:\n",
      " Epoch: 6, Validation time: 0.64s \n",
      " Loss: 54.1072: \n",
      " Recall@20: 0.0192 \n",
      " NDCG@20: 0.1051\n",
      "Epoch: 7, Training time: 2.61s, Loss: 53.8340\n",
      "Evaluate current model:\n",
      " Epoch: 7, Validation time: 0.62s \n",
      " Loss: 53.8340: \n",
      " Recall@20: 0.0234 \n",
      " NDCG@20: 0.1253\n",
      "Epoch: 8, Training time: 2.61s, Loss: 53.3746\n",
      "Evaluate current model:\n",
      " Epoch: 8, Validation time: 0.62s \n",
      " Loss: 53.3746: \n",
      " Recall@20: 0.0267 \n",
      " NDCG@20: 0.1378\n",
      "Epoch: 9, Training time: 2.60s, Loss: 52.6101\n",
      "Evaluate current model:\n",
      " Epoch: 9, Validation time: 0.62s \n",
      " Loss: 52.6101: \n",
      " Recall@20: 0.0462 \n",
      " NDCG@20: 0.1734\n",
      "Epoch: 10, Training time: 2.60s, Loss: 51.3430\n",
      "Evaluate current model:\n",
      " Epoch: 10, Validation time: 0.61s \n",
      " Loss: 51.3430: \n",
      " Recall@20: 0.0550 \n",
      " NDCG@20: 0.1955\n",
      "Epoch: 11, Training time: 2.59s, Loss: 49.0804\n",
      "Evaluate current model:\n",
      " Epoch: 11, Validation time: 0.61s \n",
      " Loss: 49.0804: \n",
      " Recall@20: 0.0654 \n",
      " NDCG@20: 0.2305\n",
      "Epoch: 12, Training time: 2.59s, Loss: 46.1354\n",
      "Evaluate current model:\n",
      " Epoch: 12, Validation time: 0.60s \n",
      " Loss: 46.1354: \n",
      " Recall@20: 0.0783 \n",
      " NDCG@20: 0.2420\n",
      "Epoch: 13, Training time: 2.62s, Loss: 43.5063\n",
      "Evaluate current model:\n",
      " Epoch: 13, Validation time: 0.61s \n",
      " Loss: 43.5063: \n",
      " Recall@20: 0.0883 \n",
      " NDCG@20: 0.2885\n",
      "Epoch: 14, Training time: 2.65s, Loss: 41.6688\n",
      "Evaluate current model:\n",
      " Epoch: 14, Validation time: 0.61s \n",
      " Loss: 41.6688: \n",
      " Recall@20: 0.0931 \n",
      " NDCG@20: 0.2887\n",
      "Epoch: 15, Training time: 2.63s, Loss: 40.4350\n",
      "Evaluate current model:\n",
      " Epoch: 15, Validation time: 0.61s \n",
      " Loss: 40.4350: \n",
      " Recall@20: 0.1025 \n",
      " NDCG@20: 0.3197\n",
      "Epoch: 16, Training time: 2.59s, Loss: 39.6296\n",
      "Evaluate current model:\n",
      " Epoch: 16, Validation time: 0.60s \n",
      " Loss: 39.6296: \n",
      " Recall@20: 0.1104 \n",
      " NDCG@20: 0.3280\n",
      "Epoch: 17, Training time: 2.58s, Loss: 38.9949\n",
      "Evaluate current model:\n",
      " Epoch: 17, Validation time: 0.60s \n",
      " Loss: 38.9949: \n",
      " Recall@20: 0.1170 \n",
      " NDCG@20: 0.3449\n",
      "Epoch: 18, Training time: 2.58s, Loss: 38.3855\n",
      "Evaluate current model:\n",
      " Epoch: 18, Validation time: 0.60s \n",
      " Loss: 38.3855: \n",
      " Recall@20: 0.1353 \n",
      " NDCG@20: 0.3655\n",
      "Epoch: 19, Training time: 2.58s, Loss: 37.8637\n",
      "Evaluate current model:\n",
      " Epoch: 19, Validation time: 0.60s \n",
      " Loss: 37.8637: \n",
      " Recall@20: 0.1389 \n",
      " NDCG@20: 0.3736\n",
      "Epoch: 20, Training time: 2.60s, Loss: 37.3879\n",
      "Evaluate current model:\n",
      " Epoch: 20, Validation time: 0.61s \n",
      " Loss: 37.3879: \n",
      " Recall@20: 0.1392 \n",
      " NDCG@20: 0.3808\n",
      "Epoch: 21, Training time: 2.64s, Loss: 37.0668\n",
      "Evaluate current model:\n",
      " Epoch: 21, Validation time: 0.61s \n",
      " Loss: 37.0668: \n",
      " Recall@20: 0.1437 \n",
      " NDCG@20: 0.3934\n",
      "Epoch: 22, Training time: 2.60s, Loss: 36.6849\n",
      "Evaluate current model:\n",
      " Epoch: 22, Validation time: 0.61s \n",
      " Loss: 36.6849: \n",
      " Recall@20: 0.1528 \n",
      " NDCG@20: 0.4107\n"
     ]
    }
   ],
   "source": [
    "# current best metric\n",
    "cur_best_metric = 0\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Set values for early stopping\n",
    "cur_best_loss, stopping_step, should_stop = 1e3, 0, False\n",
    "today = datetime.now()\n",
    "\n",
    "print(\"Start at \" + str(today))\n",
    "print(\"Using \" + str(device) + \" for computations\")\n",
    "print(\"Params on CUDA: \" + str(next(model.parameters()).is_cuda))\n",
    "\n",
    "results = {\"Epoch\": [],\n",
    "               \"Loss\": [],\n",
    "               \"Recall\": [],\n",
    "               \"NDCG\": [],\n",
    "               \"Training Time\": []}\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    t1 = time()\n",
    "    loss = train(model, data_generator, optimizer)\n",
    "    training_time = time()-t1\n",
    "    print(\"Epoch: {}, Training time: {:.2f}s, Loss: {:.4f}\".\n",
    "            format(epoch, training_time, loss))\n",
    "    \n",
    "    # print test evaluation metrics every N epochs (provided by eval_N)\n",
    "    if epoch % eval_N  == (eval_N - 1):\n",
    "        with torch.no_grad():\n",
    "            t2 = time()\n",
    "            recall, ndcg = eval_model(model.u_g_embeddings.detach(),\n",
    "                                          model.i_g_embeddings.detach(),\n",
    "                                          data_generator.R_train,\n",
    "                                          data_generator.R_test,\n",
    "                                          k)\n",
    "        print(\n",
    "                \"Evaluate current model:\\n\",\n",
    "                \"Epoch: {}, Validation time: {:.2f}s\".format(epoch, time()-t2),\"\\n\",\n",
    "                \"Loss: {:.4f}:\".format(loss), \"\\n\",\n",
    "                \"Recall@{}: {:.4f}\".format(k, recall), \"\\n\",\n",
    "                \"NDCG@{}: {:.4f}\".format(k, ndcg)\n",
    "                )\n",
    "        cur_best_metric, stopping_step, should_stop = \\\n",
    "        early_stopping(recall, cur_best_metric, stopping_step, flag_step=5)\n",
    "        \n",
    "        # save results in dict\n",
    "        results['Epoch'].append(epoch)\n",
    "        results['Loss'].append(loss)\n",
    "        results['Recall'].append(recall.item())\n",
    "        results['NDCG'].append(ndcg.item())\n",
    "        results['Training Time'].append(training_time)\n",
    "    else:\n",
    "        # save results in dict\n",
    "        results['Epoch'].append(epoch)\n",
    "        results['Loss'].append(loss)\n",
    "        results['Recall'].append(None)\n",
    "        results['NDCG'].append(None)\n",
    "        results['Training Time'].append(training_time)\n",
    "    if should_stop == True: break\n",
    "\n",
    "        \n",
    "# save\n",
    "if save_results:\n",
    "    date = today.strftime(\"%d%m%Y_%H%M\") \n",
    "    \n",
    "    # save model as .pt file\n",
    "    if os.path.isdir(\"./models\"):\n",
    "        torch.save(model.state_dict(), \"./models/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".pt\")\n",
    "    else:\n",
    "        os.mkdir(\"./models\")\n",
    "        torch.save(model.state_dict(), \"./models/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".pt\")\n",
    "\n",
    "    # save results as pandas dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.set_index('Epoch', inplace=True)\n",
    "    if os.path.isdir(\"./results\"):\n",
    "        results_df.to_csv(\"./results/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".csv\")\n",
    "    else:\n",
    "        os.mkdir(\"./results\")\n",
    "        results_df.to_csv(\"./results/\" + str(date) + \"_\" + modelname + \"_\" + dataset + \".csv\")\n",
    "    # plot loss\n",
    "    results_df['Loss'].plot(figsize=(12,8), title='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
