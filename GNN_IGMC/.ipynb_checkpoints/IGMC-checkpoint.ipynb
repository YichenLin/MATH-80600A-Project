{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "# Based on the paper Inductive Matrix Completion Based on Graph Neural Networks\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import dgl \n",
    "from dgl.data.utils import download, extract_archive, get_download_dir\n",
    "from refex import extract_refex_feature\n",
    "import argparse\n",
    "\n",
    "import math \n",
    "import torch as th \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import MetricLogger, get_neighbor_nodes_labels, subgraph_extraction_labeling, load_official_trainvaltest_split\n",
    "\n",
    "from dgl.nn.pytorch import RelGraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_official_trainvaltest_split(dataset, testing=False, rating_map=None, post_rating_map=None, ratio=1.0):\n",
    "    \"\"\"\n",
    "    Loads official train/test split and uses 10% of training samples for validaiton\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix. Assumes flattening happens everywhere in row-major fashion.\n",
    "    \"\"\"\n",
    "\n",
    "    sep = '\\t'\n",
    "\n",
    "    # Check if files exist and download otherwise\n",
    "    files = ['/u1.base', '/u1.test', '/u.item', '/u.user']\n",
    "    fname = dataset\n",
    "    data_dir = 'data/' + fname\n",
    "\n",
    "\n",
    "    dtypes = {\n",
    "        'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "        'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "    filename_train = 'data/' + dataset + '/u1.base'\n",
    "    filename_test = 'data/' + dataset + '/u1.test'\n",
    "\n",
    "    data_train = pd.read_csv(\n",
    "        filename_train, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "    data_test = pd.read_csv(\n",
    "        filename_test, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "    data_array_train = data_train.values.tolist()\n",
    "    data_array_train = np.array(data_array_train)\n",
    "    data_array_test = data_test.values.tolist()\n",
    "    data_array_test = np.array(data_array_test)\n",
    "\n",
    "    if ratio < 1.0:\n",
    "        data_array_train = data_array_train[data_array_train[:, -1].argsort()[:int(ratio*len(data_array_train))]]\n",
    "\n",
    "    data_array = np.concatenate([data_array_train, data_array_test], axis=0)\n",
    "\n",
    "    u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "    v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "    ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "    if rating_map is not None:\n",
    "        for i, x in enumerate(ratings):\n",
    "            ratings[i] = rating_map[x]\n",
    "\n",
    "    u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "    v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "    u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "    ratings = ratings.astype(np.float64)\n",
    "\n",
    "    u_nodes = u_nodes_ratings\n",
    "    v_nodes = v_nodes_ratings\n",
    "\n",
    "    neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "    # assumes that ratings_train contains at least one example of every rating type\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "\n",
    "    for i in range(len(u_nodes)):\n",
    "        assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges, see cf-nade code\n",
    "\n",
    "    num_train = data_array_train.shape[0]\n",
    "    num_test = data_array_test.shape[0]\n",
    "    num_val = int(np.ceil(num_train * 0.2))\n",
    "    num_train = num_train - num_val\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    for i in range(len(ratings)):\n",
    "        assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "    idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "    pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "    pairs_nonzero_test = pairs_nonzero[num_train+num_val:]\n",
    "\n",
    "    # Internally shuffle training set (before splitting off validation set)\n",
    "    rand_idx = list(range(len(idx_nonzero_train)))\n",
    "    np.random.seed(1234)\n",
    "    np.random.shuffle(rand_idx)\n",
    "    idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "    pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "    idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "    pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "    val_idx = idx_nonzero[0:num_val]\n",
    "    train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    assert(len(test_idx) == num_test)\n",
    "\n",
    "    val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "    train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "    \n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    if post_rating_map is None:\n",
    "        rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    else:\n",
    "        rating_mx_train[train_idx] = np.array([post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    if dataset =='ml-100k':\n",
    "\n",
    "        # movie features (genres)\n",
    "        sep = r'|'\n",
    "        movie_file = 'data/' + dataset + '/u.item'\n",
    "        movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "                         'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                         'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                         'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                         'Thriller', 'War', 'Western']\n",
    "        movie_df = pd.read_csv(movie_file, sep=sep, header=None,\n",
    "                               names=movie_headers, engine='python')\n",
    "\n",
    "        genre_headers = movie_df.columns.values[6:]\n",
    "        num_genres = genre_headers.shape[0]\n",
    "\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "            # check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                v_features[v_dict[movie_id], :] = g_vec\n",
    "\n",
    "        # user features\n",
    "\n",
    "        sep = r'|'\n",
    "        users_file = 'data/' + dataset + '/u.user'\n",
    "        users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        occupation = set(users_df['occupation'].values.tolist())\n",
    "\n",
    "        age = users_df['age'].values\n",
    "        age_max = age.max()\n",
    "\n",
    "        gender_dict = {'M': 0., 'F': 1.}\n",
    "        occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "        num_feats = 2 + len(occupation_dict)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user id']\n",
    "            if u_id in u_dict.keys():\n",
    "                # age\n",
    "                u_features[u_dict[u_id], 0] = row['age'] / np.float(age_max)\n",
    "                # gender\n",
    "                u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "                # occupation\n",
    "                u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1.\n",
    "\n",
    "    \n",
    "\n",
    "    u_features = sp.csr_matrix(u_features)\n",
    "    v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    print(\"User features shape: \"+str(u_features.shape))\n",
    "    print(\"Item features shape: \"+str(v_features.shape))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data(data):\n",
    "    \"\"\"\n",
    "    Map data to proper indices in case they are not in a continues [0, N) range\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.int32 arrays\n",
    "    Returns\n",
    "    -------\n",
    "    mapped_data : np.int32 arrays\n",
    "    n : length of mapped_data\n",
    "    \"\"\"\n",
    "    uniq = list(set(data))\n",
    "\n",
    "    id_dict = {old: new for new, old in enumerate(sorted(uniq))}\n",
    "    data = np.array([id_dict[x] for x in data])\n",
    "    n = len(uniq)\n",
    "\n",
    "    return data, id_dict, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_movielens(data):\n",
    "    g_list, label_list = map(list, zip(*data))\n",
    "    g = dgl.batch(g_list)\n",
    "    g_label = th.stack(label_list)\n",
    "    return g, g_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLens(object):\n",
    "    \"\"\"MovieLens dataset used by GCMC model\n",
    "    \"\"\"\n",
    "    def __init__(self, data_name, testing=False, \n",
    "                 test_ratio=0.1, valid_ratio=0.2):\n",
    "        (\n",
    "                u_features, v_features, adj_train, train_labels, train_u_indices, train_v_indices,\n",
    "                val_labels, val_u_indices, val_v_indices, test_labels, test_u_indices, \n",
    "                test_v_indices, class_values\n",
    "            ) = load_official_trainvaltest_split(\n",
    "                'ml-100k', testing, None, None, 1.0\n",
    "            )\n",
    "            \n",
    "        self._num_user = u_features.shape[0]\n",
    "        self._num_movie = v_features.shape[0]\n",
    "\n",
    "        # reindex u and v, v nodes start after u\n",
    "        train_v_indices += self.num_user\n",
    "        val_v_indices += self.num_user\n",
    "        test_v_indices += self.num_user\n",
    "\n",
    "        self.train_rating_pairs = (th.LongTensor(train_u_indices), th.LongTensor(train_v_indices))\n",
    "        self.valid_rating_pairs = (th.LongTensor(val_u_indices), th.LongTensor(val_v_indices))\n",
    "        self.test_rating_pairs = (th.LongTensor(test_u_indices), th.LongTensor(test_v_indices))\n",
    "        self.train_rating_values = th.FloatTensor(train_labels)\n",
    "        self.valid_rating_values = th.FloatTensor(val_labels)\n",
    "        self.test_rating_values = th.FloatTensor(test_labels)\n",
    "\n",
    "        print(\"\\tTrain rating pairs : {}\".format(len(train_labels)))\n",
    "        print(\"\\tValid rating pairs : {}\".format(len(val_labels)))\n",
    "        print(\"\\tTest rating pairs  : {}\".format(len(test_labels)))\n",
    "\n",
    "        # build dgl graph object, which is homogeneous and bidirectional and contains only training edges\n",
    "        self.train_graph = dgl.graph((th.cat([self.train_rating_pairs[0], self.train_rating_pairs[1]]), \n",
    "                                      th.cat([self.train_rating_pairs[1], self.train_rating_pairs[0]])))\n",
    "        self.train_graph.edata['etype'] = th.cat([self.train_rating_values, self.train_rating_values]).to(th.long)                    \n",
    "\n",
    "    @property\n",
    "    def num_rating(self):\n",
    "        return self._rating.size\n",
    "\n",
    "    @property\n",
    "    def num_user(self):\n",
    "        return self._num_user\n",
    "\n",
    "    @property\n",
    "    def num_movie(self):\n",
    "        return self._num_movie\n",
    "\n",
    "\n",
    "    def _generate_pair_value(self, rating_data):\n",
    "        rating_pairs = (np.array([self._global_user_id_map[ele] for ele in rating_data[\"user_id\"]],\n",
    "                                 dtype=np.int32),\n",
    "                        np.array([self._global_movie_id_map[ele] for ele in rating_data[\"movie_id\"]],\n",
    "                                 dtype=np.int32))\n",
    "        # label ranges from 0. to 4.\n",
    "        rating_values = rating_data[\"rating\"].values.astype(np.float32) - 1.\n",
    "        return rating_pairs[0], rating_pairs[1], rating_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(th.utils.data.Dataset):\n",
    "    def __init__(self, links, g_labels, graph, \n",
    "                hop=1, sample_ratio=1.0, max_nodes_per_hop=200):\n",
    "        self.links = links\n",
    "        self.g_labels = g_labels\n",
    "        self.graph = graph \n",
    "\n",
    "        self.hop = hop\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.max_nodes_per_hop = max_nodes_per_hop\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.links[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u, v = self.links[0][idx], self.links[1][idx]\n",
    "        g_label = self.g_labels[idx]\n",
    "\n",
    "        subgraph = subgraph_extraction_labeling(\n",
    "            (u, v), self.graph, \n",
    "            hop=self.hop, sample_ratio=self.sample_ratio, max_nodes_per_hop=self.max_nodes_per_hop)\n",
    "\n",
    "        return subgraph, g_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User features shape: (943, 23)\n",
      "Item features shape: (1682, 18)\n",
      "\tTrain rating pairs : 64000\n",
      "\tValid rating pairs : 16000\n",
      "\tTest rating pairs  : 20000\n"
     ]
    }
   ],
   "source": [
    "movielens = MovieLens(\"ml-100k\", testing=False)\n",
    "\n",
    "train_dataset = MovieLensDataset(\n",
    "        movielens.train_rating_pairs, movielens.train_rating_values, movielens.train_graph, \n",
    "        hop=1, sample_ratio=1.0, max_nodes_per_hop=200)\n",
    "train_loader = th.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, \n",
    "                            num_workers=8, collate_fn=collate_movielens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(size, tensor):\n",
    "    bound = 1.0 / math.sqrt(size)\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_drop(graph, edge_dropout=0.2, training=True):\n",
    "    assert edge_dropout >= 0.0 and edge_dropout <= 1.0, 'Invalid dropout rate.'\n",
    "\n",
    "    if not training:\n",
    "        return graph\n",
    "\n",
    "    # set edge mask to zero in directional mode\n",
    "    src, _ = graph.edges()\n",
    "    to_drop = src.new_full((graph.number_of_edges(), ), edge_dropout, dtype=th.float)\n",
    "    to_drop = th.bernoulli(to_drop).to(th.bool)\n",
    "    graph.edata['edge_mask'][to_drop] = 0\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGMC(nn.Module):\n",
    "    # The GNN model of Inductive Graph-based Matrix Completion. \n",
    "    # Use RGCN convolution + center-nodes readout.\n",
    "    \n",
    "    def __init__(self, in_feats, gconv=RelGraphConv, latent_dim=[32, 32, 32, 32], \n",
    "                num_relations=5, num_bases=2, regression=False, edge_dropout=0.2, \n",
    "                force_undirected=False, side_features=False, n_side_features=0, \n",
    "                multiply_by=1):\n",
    "        super(IGMC, self).__init__()\n",
    "\n",
    "        self.regression = regression\n",
    "        self.edge_dropout = edge_dropout\n",
    "        self.force_undirected = force_undirected\n",
    "        self.side_features = side_features\n",
    "        self.multiply_by = multiply_by\n",
    "\n",
    "        self.convs = th.nn.ModuleList()\n",
    "        self.convs.append(gconv(in_feats, latent_dim[0], num_relations, num_bases=num_bases, self_loop=True, low_mem=True))\n",
    "        for i in range(0, len(latent_dim)-1):\n",
    "            self.convs.append(gconv(latent_dim[i], latent_dim[i+1], num_relations, num_bases=num_bases, self_loop=True, low_mem=True))\n",
    "        \n",
    "        self.lin1 = nn.Linear(2 * sum(latent_dim), 128)\n",
    "        if side_features:\n",
    "            self.lin1 = nn.Linear(2 * sum(latent_dim) + n_side_features, 128)\n",
    "        if self.regression:\n",
    "            self.lin2 = nn.Linear(128, 1)\n",
    "        else:\n",
    "            assert False\n",
    "            # self.lin2 = nn.Linear(128, n_classes)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            size = conv.num_bases * conv.in_feat\n",
    "            uniform(size, conv.weight)\n",
    "            uniform(size, conv.w_comp)\n",
    "            uniform(size, conv.loop_weight)\n",
    "            uniform(size, conv.h_bias)\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    # @profile\n",
    "    def forward(self, block):\n",
    "        block = edge_drop(block, self.edge_dropout, self.training)\n",
    "\n",
    "        concat_states = []\n",
    "        x = block.ndata['x']\n",
    "        for conv in self.convs:\n",
    "            # edge mask zero denotes the edge dropped\n",
    "            x = th.tanh(conv(block, x, block.edata['etype'], \n",
    "                             norm=block.edata['edge_mask'].unsqueeze(1)))\n",
    "            concat_states.append(x)\n",
    "        concat_states = th.cat(concat_states, 1)\n",
    "        \n",
    "        users = block.ndata['nlabel'][:, 0] == 1\n",
    "        items = block.ndata['nlabel'][:, 1] == 1\n",
    "        x = th.cat([concat_states[users], concat_states[items]], 1)\n",
    "        # if self.side_features:\n",
    "        #     x = th.cat([x, data.u_feature, data.v_feature], 1)\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        if self.regression:\n",
    "            return x[:, 0] * self.multiply_by\n",
    "        else:\n",
    "            assert False\n",
    "            # return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    # Evaluate RMSE\n",
    "    model.eval()\n",
    "    mse = 0.\n",
    "    for batch in loader:\n",
    "        with th.no_grad():\n",
    "            preds = model(batch[0].to(device))\n",
    "        labels = batch[1].to(device)\n",
    "        mse += ((preds - labels) ** 2).sum().item()\n",
    "    mse /= len(loader.dataset)\n",
    "    return np.sqrt(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_rating_reg(model):\n",
    "    arr_loss = 0\n",
    "    for conv in model.convs:\n",
    "        weight = conv.weight.view(conv.num_bases, conv.in_feat * conv.out_feat)\n",
    "        weight = th.matmul(conv.w_comp, weight).view(conv.num_rels, conv.in_feat, conv.out_feat)\n",
    "        arr_loss += th.sum((weight[1:, :, :] - weight[:-1, :, :])**2)\n",
    "    return arr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loss_fn, optimizer, arr_lambda, loader, device, log_interval):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0.\n",
    "    iter_loss = 0.\n",
    "    iter_mse = 0.\n",
    "    iter_cnt = 0\n",
    "    iter_dur = []\n",
    "\n",
    "    for iter_idx, batch in enumerate(loader, start=1):\n",
    "        t_start = time.time()\n",
    "\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        preds = model(inputs)\n",
    "        loss = loss_fn(preds, labels).mean() + arr_lambda * adj_rating_reg(model)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * preds.shape[0]\n",
    "        iter_loss += loss.item() * preds.shape[0]\n",
    "        iter_mse += ((preds - labels) ** 2).sum().item()\n",
    "        iter_cnt += preds.shape[0]\n",
    "        iter_dur.append(time.time() - t_start)\n",
    "\n",
    "        if iter_idx % log_interval == 0:\n",
    "            print(\"Iter={}, loss={:.4f}, rmse={:.4f}, time={:.4f}\".format(\n",
    "                iter_idx, iter_loss/iter_cnt, iter_mse/iter_cnt, np.average(iter_dur)))\n",
    "            iter_loss = 0.\n",
    "            iter_mse = 0.\n",
    "            iter_cnt = 0\n",
    "\n",
    "    return epoch_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    ### prepare data and set model\n",
    "    movielens = MovieLens(args.data_name, testing=args.testing,\n",
    "                            test_ratio=args.data_test_ratio, valid_ratio=args.data_valid_ratio)\n",
    "    if args.testing:\n",
    "        test_dataset = MovieLensDataset(\n",
    "            movielens.test_rating_pairs, movielens.test_rating_values, movielens.train_graph, \n",
    "            args.hop, args.sample_ratio, args.max_nodes_per_hop) \n",
    "    else:\n",
    "        test_dataset = MovieLensDataset(\n",
    "            movielens.valid_rating_pairs, movielens.valid_rating_values, movielens.train_graph, \n",
    "            args.hop, args.sample_ratio, args.max_nodes_per_hop)\n",
    "    train_dataset = MovieLensDataset(\n",
    "        movielens.train_rating_pairs, movielens.train_rating_values, movielens.train_graph, \n",
    "        args.hop, args.sample_ratio, args.max_nodes_per_hop)\n",
    "\n",
    "    train_loader = th.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, \n",
    "                            num_workers=args.num_workers, collate_fn=collate_movielens)\n",
    "    test_loader = th.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, \n",
    "                            num_workers=args.num_workers, collate_fn=collate_movielens)\n",
    "\n",
    "    in_feats = (args.hop+1)*2 #+ movielens.train_graph.ndata['refex'].shape[1]\n",
    "    model = IGMC(in_feats=in_feats, \n",
    "                 latent_dim=[32, 32, 32, 32],\n",
    "                 num_relations=5, # movielens.num_rating, \n",
    "                 num_bases=4, \n",
    "                 regression=True, \n",
    "                 edge_dropout=args.edge_dropout,\n",
    "                #  side_features=args.use_features,\n",
    "                #  n_side_features=n_features,\n",
    "                #  multiply_by=args.multiply_by\n",
    "            ).to(args.device)\n",
    "    loss_fn = nn.MSELoss().to(args.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.train_lr, weight_decay=0)\n",
    "    print(\"Loading network finished ...\\n\")\n",
    "\n",
    "    ### prepare the logger\n",
    "    logger = MetricLogger(args.save_dir, args.valid_log_interval)\n",
    "    \n",
    "    best_epoch = 0\n",
    "    best_rmse = np.inf\n",
    "    ### declare the loss information\n",
    "    print(\"Start training ...\")\n",
    "    for epoch_idx in range(1, args.train_epochs+1):\n",
    "        print ('Epoch', epoch_idx)\n",
    "    \n",
    "        train_loss = train_epoch(model, loss_fn, optimizer, args.arr_lambda, \n",
    "                                train_loader, args.device, args.train_log_interval)\n",
    "        test_rmse = evaluate(model, test_loader, args.device)\n",
    "        eval_info = {\n",
    "            'epoch': epoch_idx,\n",
    "            'train_loss': train_loss,\n",
    "            'test_rmse': test_rmse,\n",
    "        }\n",
    "        print('=== Epoch {}, train loss {:.6f}, test rmse {:.6f} ==='.format(*eval_info.values()))\n",
    "\n",
    "        if epoch_idx % args.train_lr_decay_step == 0:\n",
    "            for param in optimizer.param_groups:\n",
    "                param['lr'] = args.train_lr_decay_factor * param['lr']\n",
    "\n",
    "        logger.log(eval_info, model, optimizer)\n",
    "        if best_rmse > test_rmse:\n",
    "            best_rmse = test_rmse\n",
    "            best_epoch = epoch_idx\n",
    "    eval_info = \"Training ends. The best testing rmse is {:.6f} at epoch {}\".format(best_rmse, best_epoch)\n",
    "    print(eval_info)\n",
    "    with open(os.path.join(args.save_dir, 'log.txt'), 'a') as f:\n",
    "        f.write(eval_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config():\n",
    "    args = argparse.ArgumentParser(description='IGMC')\n",
    "    # general settings\n",
    "    args.testing = False\n",
    "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    args.seed = 1234\n",
    "    args.data_name = 'ml-100k'\n",
    "    args.data_test_ratio = 0.1\n",
    "    args.num_workers = 0\n",
    "    args.data_valid_ratio = 0.2\n",
    "    args.train_log_interval = 100\n",
    "    args.valid_log_interval = 100\n",
    "    args.save_appendix = 'debug'\n",
    "    \n",
    "    # subgraph extraction settings\n",
    "    args.hop = 1\n",
    "    args.sample_ratio = 1.0\n",
    "    args.max_nodes_per_hop = 200\n",
    "    \n",
    "    # edge dropout settings\n",
    "    args.edge_dropout = 0.2\n",
    "    args.force_undirected = False\n",
    "    \n",
    "    # optimization settings\n",
    "    args.train_lr = 1e-3\n",
    "    args.train_min_lr = 1e-6\n",
    "    args.train_lr_decay_factor = 0.1\n",
    "    args.train_lr_decay_step = 50\n",
    "    args.train_epochs = 80\n",
    "    args.batch_size = 32\n",
    "    args.arr_lambda = 0.001\n",
    "    args.num_rgcn_bases = 4\n",
    "   \n",
    "    ## set save_dir according to localtime and test mode\n",
    "    file_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    val_test_appendix = 'testmode' if args.testing else 'valmode'\n",
    "    local_time = time.strftime('%y%m%d%H%M', time.localtime())\n",
    "    args.save_dir = os.path.join(\n",
    "        file_dir, 'log/{}_{}_{}_{}'.format(\n",
    "            args.data_name, args.save_appendix, val_test_appendix, local_time\n",
    "        )\n",
    "    )\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir) \n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User features shape: (943, 23)\n",
      "Item features shape: (1682, 18)\n",
      "\tTrain rating pairs : 64000\n",
      "\tValid rating pairs : 16000\n",
      "\tTest rating pairs  : 20000\n",
      "Loading network finished ...\n",
      "\n",
      "Start training ...\n",
      "Epoch 1\n",
      "Iter=100, loss=1.5868, rmse=1.5848, time=0.0185\n",
      "Iter=200, loss=1.1213, rmse=1.1193, time=0.0176\n",
      "Iter=300, loss=1.1604, rmse=1.1585, time=0.0173\n",
      "Iter=400, loss=1.1105, rmse=1.1086, time=0.0172\n",
      "Iter=500, loss=1.0613, rmse=1.0594, time=0.0172\n",
      "Iter=600, loss=1.1472, rmse=1.1454, time=0.0171\n",
      "Iter=700, loss=1.0562, rmse=1.0545, time=0.0170\n",
      "Iter=800, loss=1.0601, rmse=1.0584, time=0.0170\n",
      "Iter=900, loss=1.0738, rmse=1.0722, time=0.0169\n",
      "Iter=1000, loss=1.0589, rmse=1.0572, time=0.0168\n",
      "Iter=1100, loss=1.0619, rmse=1.0603, time=0.0168\n",
      "Iter=1200, loss=1.0292, rmse=1.0277, time=0.0168\n",
      "Iter=1300, loss=1.0334, rmse=1.0319, time=0.0167\n",
      "Iter=1400, loss=1.0150, rmse=1.0136, time=0.0167\n",
      "Iter=1500, loss=1.0436, rmse=1.0422, time=0.0167\n",
      "Iter=1600, loss=0.9932, rmse=0.9918, time=0.0167\n",
      "Iter=1700, loss=1.0070, rmse=1.0056, time=0.0166\n",
      "Iter=1800, loss=1.0452, rmse=1.0439, time=0.0166\n",
      "Iter=1900, loss=0.9824, rmse=0.9810, time=0.0166\n",
      "Iter=2000, loss=1.0344, rmse=1.0331, time=0.0166\n",
      "=== Epoch 1, train loss 1.084092, test rmse 0.939933 ===\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/yichen/Dropbox/MATH80600/MATH-80600A-Project/GNN_IGMC/log/ml-100k_debug_valmode_2104110059/log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b8fc129d44e2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_lr_decay_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_rmse\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtest_rmse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mbest_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_rmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/MATH80600/MATH-80600A-Project/GNN_IGMC/utils.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(self, info, model, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_rmse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'log.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             f.write('Epoch {}, train loss {:.4f}, test rmse {:.6f}\\n'.format(\n\u001b[1;32m     20\u001b[0m                 epoch, train_loss, test_rmse))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/yichen/Dropbox/MATH80600/MATH-80600A-Project/GNN_IGMC/log/ml-100k_debug_valmode_2104110059/log.txt'"
     ]
    }
   ],
   "source": [
    "train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
