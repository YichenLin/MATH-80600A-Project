{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aquatic-spencer",
   "metadata": {},
   "source": [
    "# Import the packages and check connection to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seventh-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd ## for dataset and eda\n",
    "import numpy as np ## for eda\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "buried-virginia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Bucket: firstprojectdl>\n",
      "Great, we now have access to our first bucket on google cloud storage where we put our data\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"firstprojectdl\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "print(bucket)\n",
    "print('Great, we now have access to our first bucket on google cloud storage where we put our data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "controlling-camel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings100k.dat\n",
      "u.data\n",
      "u.item\n",
      "u.user\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "\n",
    "bucket_name = \"firstprojectdl\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "# When you have your files in a subfolder of the bucket.\n",
    "my_prefix = \"data/movieLens/movieLens100k/\" # the name of the subfolder\n",
    "blobs = bucket.list_blobs(prefix = my_prefix, delimiter = '/')\n",
    "\n",
    "dfDict = {}\n",
    "dateparse = lambda x: datetime.utcfromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "for blob in blobs:\n",
    "    if(blob.name != my_prefix): # ignoring the subfolder itself \n",
    "        file_name = blob.name.replace(my_prefix, \"\")\n",
    "        blob.download_to_filename(file_name) # download the file to the machine\n",
    "        print(file_name)\n",
    "        if file_name =='ratings100k.dat':\n",
    "            df = pd.read_csv(file_name, sep='\\t', \n",
    "                            names=['user_id', 'movie_id', 'rating', 'timestamp'], \n",
    "                            parse_dates=['timestamp'], \n",
    "                            date_parser=dateparse) # load the rating data\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "apparent-planner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>1997-12-04 15:55:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-04-04 19:22:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-11-07 07:18:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-11-27 05:02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-02-02 05:33:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating           timestamp\n",
       "0      196       242       3 1997-12-04 15:55:49\n",
       "1      186       302       3 1998-04-04 19:22:22\n",
       "2       22       377       1 1997-11-07 07:18:36\n",
       "3      244        51       2 1997-11-27 05:02:03\n",
       "4      166       346       1 1998-02-02 05:33:16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "polyphonic-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ratings100k.dat') as infile:\n",
    "  with open('ratings100kNew.dat', 'w') as outfile:\n",
    "    for line in infile:\n",
    "      fields = line.split('\\t')\n",
    "      outfile.write('::'.join(fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "greek-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "remarkable-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comet ML for experiment logging\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "light-intervention",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "diagnostic-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_random_seed(1994)\n",
    "np.random.seed(1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "painted-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rating(num_users, num_items, num_total_ratings, a, b, train_ratio):\n",
    "    \"\"\"\n",
    "    Function to read in the ratings data\n",
    "    :param path: path to ratings data\n",
    "    :param num_users: number of users\n",
    "    :param num_items: number of items\n",
    "    :param num_total_ratings: number of total ratings\n",
    "    :param a: positive rating (1)\n",
    "    :param b: negative rating (0)\n",
    "    :param train_ratio: ratio that splits train and test sets\n",
    "    \"\"\"\n",
    "    fp = open(\"ratings100kNew.dat\")\n",
    "\n",
    "    user_train_set = set()\n",
    "    user_test_set = set()\n",
    "    item_train_set = set()\n",
    "    item_test_set = set()\n",
    "\n",
    "    R = np.zeros((num_users, num_items))\n",
    "    mask_R = np.zeros((num_users, num_items))\n",
    "    C = np.ones((num_users, num_items)) * b\n",
    "\n",
    "    train_R = np.zeros((num_users, num_items))\n",
    "    test_R = np.zeros((num_users, num_items))\n",
    "\n",
    "    train_mask_R = np.zeros((num_users, num_items))\n",
    "    test_mask_R = np.zeros((num_users, num_items))\n",
    "\n",
    "    random_perm_idx = np.random.permutation(num_total_ratings)\n",
    "    train_idx = random_perm_idx[0:int(num_total_ratings * train_ratio)]\n",
    "    test_idx = random_perm_idx[int(num_total_ratings * train_ratio):]\n",
    "\n",
    "    num_train_ratings = len(train_idx)\n",
    "    num_test_ratings = len(test_idx)\n",
    "\n",
    "    lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        user, item, rating, _ = line.split(\"::\")\n",
    "        user_idx = int(user) - 1\n",
    "        item_idx = int(item) - 1\n",
    "        R[user_idx, item_idx] = int(rating)\n",
    "        mask_R[user_idx, item_idx] = 1\n",
    "        C[user_idx, item_idx] = a\n",
    "\n",
    "    # Training set\n",
    "    for itr in train_idx:\n",
    "        line = lines[itr]\n",
    "        user, item, rating, _ = line.split(\"::\")\n",
    "        user_idx = int(user) - 1\n",
    "        item_idx = int(item) - 1\n",
    "        train_R[user_idx, item_idx] = int(rating)\n",
    "        train_mask_R[user_idx, item_idx] = 1\n",
    "\n",
    "        user_train_set.add(user_idx)\n",
    "        item_train_set.add(item_idx)\n",
    "\n",
    "    # Test set\n",
    "    for itr in test_idx:\n",
    "        line = lines[itr]\n",
    "        user, item, rating, _ = line.split(\"::\")\n",
    "        user_idx = int(user) - 1\n",
    "        item_idx = int(item) - 1\n",
    "        test_R[user_idx, item_idx] = int(rating)\n",
    "        test_mask_R[user_idx, item_idx] = 1\n",
    "\n",
    "        user_test_set.add(user_idx)\n",
    "        item_test_set.add(item_idx)\n",
    "\n",
    "    return R, mask_R, C, train_R, train_mask_R, test_R, test_mask_R, num_train_ratings, num_test_ratings, \\\n",
    "           user_train_set, item_train_set, user_test_set, item_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "little-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'ml-1ook'\n",
    "num_users = 943\n",
    "num_items = 1683\n",
    "num_total_ratings = 100000\n",
    "# Data is split into random 75% - 25% train-test sets\n",
    "train_ratio = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "multiple-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "R, mask_R, C, train_R, train_mask_R, test_R, test_mask_R, num_train_ratings, num_test_ratings, \\\n",
    "user_train_set, item_train_set, user_test_set, item_test_set = read_rating(num_users, num_items, num_total_ratings, 1, 0, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quarterly-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hyper-parameters\n",
    "hyper_params = {\n",
    "    \"hidden_neuron\": 500,\n",
    "    \"lambda_value\": 1,\n",
    "    \"epochs\": 500,\n",
    "    \"batch_size\": 512,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"random_seed\": 1994\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "former-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "virgin-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRec:\n",
    "    \"\"\"\n",
    "    Function to define the AutoRec model class\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, args,\n",
    "                 num_users, num_items,\n",
    "                 R, mask_R, C, train_R, train_mask_R, test_R, test_mask_R, num_train_ratings, num_test_ratings,\n",
    "                 user_train_set, item_train_set, user_test_set, item_test_set,\n",
    "                 result_path):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.args = args\n",
    "\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "\n",
    "        self.R = R\n",
    "        self.mask_R = mask_R\n",
    "        self.C = C\n",
    "        self.train_R = train_R\n",
    "        self.train_mask_R = train_mask_R\n",
    "        self.test_R = test_R\n",
    "        self.test_mask_R = test_mask_R\n",
    "        self.num_train_ratings = num_train_ratings\n",
    "        self.num_test_ratings = num_test_ratings\n",
    "\n",
    "        self.user_train_set = user_train_set\n",
    "        self.item_train_set = item_train_set\n",
    "        self.user_test_set = user_test_set\n",
    "        self.item_test_set = item_test_set\n",
    "\n",
    "        self.hidden_neuron = args['hidden_neuron']\n",
    "        self.train_epoch = args['train_epoch']\n",
    "        self.batch_size = args['batch_size']\n",
    "        self.num_batch = int(math.ceil(self.num_users / float(self.batch_size)))\n",
    "\n",
    "        self.base_lr = args['base_lr']\n",
    "        self.optimizer_method = args['optimizer_method']\n",
    "        self.display_step = args['display_step']\n",
    "        self.random_seed = args['random_seed']\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.decay_epoch_step = args['decay_epoch_step']\n",
    "        self.decay_step = self.decay_epoch_step * self.num_batch\n",
    "        self.lr = tf.compat.v1.train.exponential_decay(self.base_lr, self.global_step,\n",
    "                                                       self.decay_step, 0.96, staircase=True)\n",
    "        self.lambda_value = args['lambda_value']\n",
    "\n",
    "        self.train_cost_list = []\n",
    "        self.test_cost_list = []\n",
    "        self.test_rmse_list = []\n",
    "\n",
    "        self.result_path = result_path\n",
    "        self.grad_clip = args['grad_clip']\n",
    "\n",
    "    def run(self, experiment):\n",
    "        \"\"\"\n",
    "        Function to run AutoRec\n",
    "        :param experiment: CometML Experiment function\n",
    "        \"\"\"\n",
    "        # Build AutoRec\n",
    "        self.prepare_model()\n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "        # Train and evaluate AutoRec for all epochs\n",
    "        for epoch_itr in range(self.train_epoch):\n",
    "            experiment.set_step(epoch_itr)\n",
    "            self.train_model(epoch_itr,experiment)\n",
    "            self.test_model(epoch_itr, experiment)\n",
    "\n",
    "        # Log results\n",
    "        self.make_records()\n",
    "\n",
    "    def prepare_model(self):\n",
    "        \"\"\"\n",
    "        Function to build AutoRec\n",
    "        \"\"\"\n",
    "        self.input_R = tf.compat.v1.placeholder(dtype=tf.float32,\n",
    "                                                shape=[None, self.num_items],\n",
    "                                                name=\"input_R\")\n",
    "        self.input_mask_R = tf.compat.v1.placeholder(dtype=tf.float32,\n",
    "                                                     shape=[None, self.num_items],\n",
    "                                                     name=\"input_mask_R\")\n",
    "\n",
    "        V = tf.compat.v1.get_variable(name=\"V\", initializer=tf.compat.v1.truncated_normal(\n",
    "            shape=[self.num_items, self.hidden_neuron],\n",
    "            mean=0, stddev=0.03), dtype=tf.float32)\n",
    "        W = tf.compat.v1.get_variable(name=\"W\", initializer=tf.compat.v1.truncated_normal(\n",
    "            shape=[self.hidden_neuron, self.num_items],\n",
    "            mean=0, stddev=0.03), dtype=tf.float32)\n",
    "        mu = tf.compat.v1.get_variable(name=\"mu\", initializer=tf.zeros(shape=self.hidden_neuron), dtype=tf.float32)\n",
    "        b = tf.compat.v1.get_variable(name=\"b\", initializer=tf.zeros(shape=self.num_items), dtype=tf.float32)\n",
    "\n",
    "        pre_Encoder = tf.matmul(self.input_R, V) + mu\n",
    "        self.Encoder = tf.nn.sigmoid(pre_Encoder)\n",
    "        pre_Decoder = tf.matmul(self.Encoder, W) + b\n",
    "        self.Decoder = tf.identity(pre_Decoder)\n",
    "\n",
    "        pre_rec_cost = tf.multiply((self.input_R - self.Decoder), self.input_mask_R)\n",
    "        rec_cost = tf.square(self.l2_norm(pre_rec_cost))\n",
    "        pre_reg_cost = tf.square(self.l2_norm(W)) + tf.square(self.l2_norm(V))\n",
    "        reg_cost = self.lambda_value * 0.5 * pre_reg_cost\n",
    "\n",
    "        self.cost = rec_cost + reg_cost\n",
    "\n",
    "        if self.optimizer_method == \"Adam\":\n",
    "            optimizer = tf.compat.v1.train.AdamOptimizer(self.lr)\n",
    "        elif self.optimizer_method == \"RMSProp\":\n",
    "            optimizer = tf.compat.v1.train.RMSPropOptimizer(self.lr)\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer Key ERROR\")\n",
    "\n",
    "        if self.grad_clip:\n",
    "            gvs = optimizer.compute_gradients(self.cost)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gvs]\n",
    "            self.optimizer = optimizer.apply_gradients(capped_gvs, global_step=self.global_step)\n",
    "        else:\n",
    "            self.optimizer = optimizer.minimize(self.cost, global_step=self.global_step)\n",
    "\n",
    "    def train_model(self, itr, experiment):\n",
    "        \"\"\"\n",
    "        Function to train AutoRec\n",
    "        :param itr: Current iteration\n",
    "        :param experiment: CometML experiment\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        random_perm_doc_idx = np.random.permutation(self.num_users)\n",
    "\n",
    "        batch_cost = 0\n",
    "        for i in range(self.num_batch):\n",
    "            if i == self.num_batch - 1:\n",
    "                batch_set_idx = random_perm_doc_idx[i * self.batch_size:]\n",
    "            elif i < self.num_batch - 1:\n",
    "                batch_set_idx = random_perm_doc_idx[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "\n",
    "            _, Cost = self.sess.run(\n",
    "                [self.optimizer, self.cost],\n",
    "                feed_dict={self.input_R: self.train_R[batch_set_idx, :],\n",
    "                           self.input_mask_R: self.train_mask_R[batch_set_idx, :]})\n",
    "\n",
    "            batch_cost = batch_cost + Cost\n",
    "        self.train_cost_list.append(batch_cost)\n",
    "\n",
    "        if (itr + 1) % self.display_step == 0:\n",
    "\n",
    "            print(\"Training //\", \"Epoch %d //\" % (itr), \" Total cost = {:.2f}\".format(batch_cost),\n",
    "                  \"Elapsed time : %d sec\" % (time.time() - start_time))\n",
    "\n",
    "        experiment.log_metric(\"loss\", batch_cost, step=itr)\n",
    "\n",
    "    def test_model(self, itr, experiment):\n",
    "        \"\"\"\n",
    "        Function to evaluate AutoRec\n",
    "        :param itr: Current iteration\n",
    "        :param experiment: CometML experiment\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        Cost, Decoder = self.sess.run(\n",
    "            [self.cost, self.Decoder],\n",
    "            feed_dict={self.input_R: self.test_R,\n",
    "                       self.input_mask_R: self.test_mask_R})\n",
    "\n",
    "        self.test_cost_list.append(Cost)\n",
    "\n",
    "        if (itr + 1) % self.display_step == 0:\n",
    "            Estimated_R = Decoder.clip(min=1, max=5)\n",
    "            unseen_user_test_list = list(self.user_test_set - self.user_train_set)\n",
    "            unseen_item_test_list = list(self.item_test_set - self.item_train_set)\n",
    "\n",
    "            for user in unseen_user_test_list:\n",
    "                for item in unseen_item_test_list:\n",
    "                    if self.test_mask_R[user, item] == 1:  # exist in test set\n",
    "                        Estimated_R[user, item] = 3\n",
    "\n",
    "            pre_numerator = np.multiply((Estimated_R - self.test_R), self.test_mask_R)\n",
    "            numerator = np.sum(np.square(pre_numerator))\n",
    "            denominator = self.num_test_ratings\n",
    "            RMSE = np.sqrt(numerator / float(denominator))\n",
    "\n",
    "            self.test_rmse_list.append(RMSE)\n",
    "\n",
    "            print(\"Testing //\", \"Epoch %d //\" % (itr), \" Total cost = {:.2f}\".format(Cost),\n",
    "                  \" RMSE = {:.5f}\".format(RMSE),\n",
    "                  \"Elapsed time : %d sec\" % (time.time() - start_time))\n",
    "            print(\"=\" * 100)\n",
    "\n",
    "        experiment.log_metric(\"RMSE\", RMSE, step=itr)\n",
    "\n",
    "    def make_records(self):\n",
    "        \"\"\"\n",
    "        Function to log results\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.result_path):\n",
    "            os.makedirs(self.result_path)\n",
    "\n",
    "        basic_info = self.result_path + \"basic_info.txt\"\n",
    "        train_record = self.result_path + \"train_record.txt\"\n",
    "        test_record = self.result_path + \"test_record.txt\"\n",
    "\n",
    "        with open(train_record, 'w') as f:\n",
    "            f.write(str(\"Cost:\"))\n",
    "            f.write('\\t')\n",
    "            for itr in range(len(self.train_cost_list)):\n",
    "                f.write(str(self.train_cost_list[itr]))\n",
    "                f.write('\\t')\n",
    "            f.write('\\n')\n",
    "\n",
    "        with open(test_record, 'w') as g:\n",
    "            g.write(str(\"Cost:\"))\n",
    "            g.write('\\t')\n",
    "            for itr in range(len(self.test_cost_list)):\n",
    "                g.write(str(self.test_cost_list[itr]))\n",
    "                g.write('\\t')\n",
    "            g.write('\\n')\n",
    "\n",
    "            g.write(str(\"RMSE:\"))\n",
    "            for itr in range(len(self.test_rmse_list)):\n",
    "                g.write(str(self.test_rmse_list[itr]))\n",
    "                g.write('\\t')\n",
    "            g.write('\\n')\n",
    "\n",
    "        #with open(basic_info, 'w') as h:\n",
    "        #    h.write(str(self.args))\n",
    "\n",
    "    def l2_norm(self, tensor):\n",
    "        \"\"\"\n",
    "        Function to apply L2 normalization\n",
    "        :param tensor: TensorFlow tensor\n",
    "        \"\"\"\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rental-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorFlow Config\n",
    "config = tf.compat.v1.ConfigProto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "indian-accordance",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/benichou/autoencoders-movielens100k/5759b090846e4451a274ab7d75b30b6c\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size    : 512\n",
      "COMET INFO:     epochs        : 500\n",
      "COMET INFO:     hidden_neuron : 500\n",
      "COMET INFO:     lambda_value  : 1\n",
      "COMET INFO:     learning_rate : 0.001\n",
      "COMET INFO:     optimizer     : Adam\n",
      "COMET INFO:     random_seed   : 1994\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (24 MB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/benichou/autoencoders-movielens100k/3a3f5313297840e1873341780c17ed6a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"vV3GGUE8tetqNJuD2kGxVi1M9\", project_name=\"autoencoders-movielens100k\")\n",
    "experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "french-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'hidden_neuron': 500, 'lambda_value':1, 'train_epoch': 500, 'batch_size': 512, 'optimizer_method': 'Adam', 'grad_clip': False, 'base_lr': 1e-3, 'decay_epoch_step': 50, 'random_seed':1994, 'display_step':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "floating-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "third-gates",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable V already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-60-3d6b2725f39f>\", line 88, in prepare_model\n    mean=0, stddev=0.03), dtype=tf.float32)\n  File \"<ipython-input-60-3d6b2725f39f>\", line 62, in run\n    self.prepare_model()\n  File \"<ipython-input-65-96b2d812faeb>\", line 9, in <module>\n    AutoRec.run(experiment)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-96b2d812faeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                       result_path)\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Run the AutoRec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mAutoRec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-3d6b2725f39f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, experiment)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \"\"\"\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Build AutoRec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-3d6b2725f39f>\u001b[0m in \u001b[0;36mprepare_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         V = tf.compat.v1.get_variable(name=\"V\", initializer=tf.compat.v1.truncated_normal(\n\u001b[1;32m     87\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_neuron\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             mean=0, stddev=0.03), dtype=tf.float32)\n\u001b[0m\u001b[1;32m     89\u001b[0m         W = tf.compat.v1.get_variable(name=\"W\", initializer=tf.compat.v1.truncated_normal(\n\u001b[1;32m     90\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_neuron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_items\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 864\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable V already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-60-3d6b2725f39f>\", line 88, in prepare_model\n    mean=0, stddev=0.03), dtype=tf.float32)\n  File \"<ipython-input-60-3d6b2725f39f>\", line 62, in run\n    self.prepare_model()\n  File \"<ipython-input-65-96b2d812faeb>\", line 9, in <module>\n    AutoRec.run(experiment)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    # Define the AutoRec class from `AutoRec.py`\n",
    "    AutoRec = AutoRec(sess, args,\n",
    "                      num_users, num_items,\n",
    "                      R, mask_R, C, train_R, train_mask_R, test_R, test_mask_R, num_train_ratings, num_test_ratings,\n",
    "                      user_train_set, item_train_set, user_test_set, item_test_set,\n",
    "                      result_path)\n",
    "    # Run the AutoRec model\n",
    "    AutoRec.run(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-queen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
